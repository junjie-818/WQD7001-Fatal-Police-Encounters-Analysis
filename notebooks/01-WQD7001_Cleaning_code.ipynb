{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Cleaning code of 'fatal-police-shootings-data'"
      ],
      "metadata": {
        "id": "I87W42GoJiVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-skifNg5G9w",
        "outputId": "25673453-9285-4dc5-90c8-5574ea7b3cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loading data from: /content/drive/MyDrive/WQD7001_Project/data/raw/fatal-police-shootings-data.csv\n",
            "Data loaded successfully with encoding 'latin-1'.\n",
            "Raw data volume：10430 records\n",
            "1.1 Data Filtering: records from 2020 through 2024\n",
            "1.2 A total of 5507 records remain after filtering and re-indexing\n",
            "2.1 Duplicate Records Handled: Removed 0 records\n",
            "Starting Age Imputation...\n",
            "3.x Age imputed using hierarchical median strategy (Race/Gender -> Race -> Global)\n",
            "3.2 Situational indicators (threat/flee/armed) and gender consolidated to unknown\n",
            "3.3 Non-core fields were unified to 'Unknown/Missing'\n",
            "3.4 Repaired and standardized irregular text values for 'city' and 'county'\n",
            "3.5 Geographical names normalized, punctuation removed, and casing standardized\n",
            "4.1 Decomposing the 'armed_with' column: 7 binary weapon features created.\n",
            "4.2 Race Feature Engineering: Six overlapping binary share features created\n",
            "5.1 Outlier Treatment Handled: Removed 0 records with conflicting 'unarmed' status\n",
            "5.2 Outlier Treatment Handled: 'age' capped at 99th percentile (74.0)\n",
            "Cleaned data volume: 5199 records\n",
            "Cleaned data saved to '/content/drive/MyDrive/WQD7001_Project/data/processed/cleaned_fatal_shootings_data_2020_2024-newmain.csv'\n",
            "6.1 Final Feature Selection: Dropped low-value/redundant columns\n",
            "Final data volume: 5199 records.\n",
            "Final data saved to '/content/drive/MyDrive/WQD7001_Project/data/processed/cleaned_selected_features_2020_2024.csv'\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/WQD7001_Project'\n",
        "raw_data_path = os.path.join(base_dir, 'data', 'raw', 'fatal-police-shootings-data.csv')\n",
        "processed_data_path = os.path.join(base_dir, 'data', 'processed')\n",
        "\n",
        "os.makedirs(processed_data_path, exist_ok=True)\n",
        "\n",
        "# Define non-core qualitative columns for uniform 'Unknown/Missing' filling\n",
        "non_core_qualitative_cols = ['county', 'latitude', 'longitude', 'location_precision', 'race_source', 'agency_ids']\n",
        "\n",
        "print(f\"Loading data from: {raw_data_path}\")\n",
        "try:\n",
        "    df = pd.read_csv(raw_data_path, encoding='latin-1')\n",
        "    print(\"Data loaded successfully with encoding 'latin-1'.\")\n",
        "except UnicodeDecodeError:\n",
        "    df = pd.read_csv(raw_data_path)\n",
        "    print(\"Data loaded with default encoding (UTF-8).\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at  {raw_data_path}\")\n",
        "\n",
        "#Ensure there are no spaces in column names and obtain the raw data volume\n",
        "df.columns = df.columns.str.strip()\n",
        "print(f\"Raw data volume：{len(df)} records\")\n",
        "initial_rows = len(df)\n",
        "\n",
        "# 1. Format Standardized\n",
        "# 1.1 Filter data from 2018 to 2024（inclusive）\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year\n",
        "df = df[(df['year'] >= 2020) & (df['year'] <= 2024)].copy()\n",
        "print(f\"1.1 Data Filtering: records from 2020 through 2024\")\n",
        "\n",
        "# 1.2 Create new row id starting from 1\n",
        "df = df.reset_index(drop=True)\n",
        "df['row_id'] = df.index + 1\n",
        "print(f\"1.2 A total of {len(df)} records remain after filtering and re-indexing\")\n",
        "\n",
        "# 2. Duplicate Records Handled\n",
        "count_before_dedupe = len(df)\n",
        "df.drop_duplicates(subset=['name', 'date', 'city', 'state'], keep='first', inplace=True)\n",
        "records_removed_by_dedupe = count_before_dedupe - len(df)\n",
        "print(f\"2.1 Duplicate Records Handled: Removed {records_removed_by_dedupe} records\")\n",
        "\n",
        "# 3. Missing Values Handled\n",
        "# 3.1 Drop missing core identifiers, situational indicators\n",
        "df.dropna(subset=['name', 'date', 'city', 'state'], inplace=True)\n",
        "\n",
        "print(\"Starting Age Imputation...\")\n",
        "df['age'] = df['age'].fillna(df.groupby(['race', 'gender'])['age'].transform('median'))\n",
        "df['age'] = df['age'].fillna(df.groupby(['race'])['age'].transform('median'))\n",
        "df['age'] = df['age'].fillna(df['age'].median())\n",
        "print(\"3.x Age imputed using hierarchical median strategy (Race/Gender -> Race -> Global)\")\n",
        "\n",
        "# 3.2 fill situational indicators with unknown (Modified based on teammate request)\n",
        "# Logic update: fillna with 'unknown', replace 'undetermined' with 'unknown'\n",
        "df['threat_type'] = df['threat_type'].fillna('unknown').replace('undetermined', 'unknown')\n",
        "df['flee_status'] = df['flee_status'].fillna('unknown').replace('undetermined', 'unknown')\n",
        "df['armed_with'] = df['armed_with'].fillna('unknown').replace('undetermined', 'unknown')\n",
        "# Logic update: gender 'non-binary' to 'unknown'\n",
        "df['gender'] = df['gender'].fillna('unknown').replace('non-binary', 'unknown')\n",
        "print(\"3.2 Situational indicators (threat/flee/armed) and gender consolidated to unknown\")\n",
        "\n",
        "# 3.3 Non-Core Qualitative Variables: Indicator Category Imputation\n",
        "for col in non_core_qualitative_cols:\n",
        "    df[col] = df[col].fillna('Unknown/Missing')\n",
        "print(f\"3.3 Non-core fields were unified to 'Unknown/Missing'\")\n",
        "\n",
        "# 3.4 Repair and standardize irregular text values for 'city' and 'county'\n",
        "df['city'] = df['city'].astype(str).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "df['county'] = df['county'].astype(str).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
        "df['city'] = df['city'].str.replace(r'asley', 'easley', regex=True, case=False)\n",
        "print(f\"3.4 Repaired and standardized irregular text values for 'city' and 'county'\")\n",
        "\n",
        "# 3.5 Standardize geographical names for consistency\n",
        "df['city'] = df['city'].str.replace(r'[.,]', '', regex=True)\n",
        "df['county'] = df['county'].str.replace(r'[.,]', '', regex=True)\n",
        "df['city'] = df['city'].str.lower().str.strip()\n",
        "df['county'] = df['county'].str.lower().str.strip()\n",
        "df['state'] = df['state'].str.upper().str.strip()\n",
        "print(\"3.5 Geographical names normalized, punctuation removed, and casing standardized\")\n",
        "\n",
        "# 4. Feature Engineering\n",
        "# 4.1 Decomposing the 'armed_with' column\n",
        "df['armed_with'] = df['armed_with'].astype(str)\n",
        "weapon_keywords_main = ['gun', 'knife', 'blunt_object', 'vehicle', 'replica']\n",
        "for keyword in weapon_keywords_main:\n",
        "    df[f'has_{keyword}'] = df['armed_with'].str.contains(keyword, case=False, na=False).astype(int)\n",
        "\n",
        "df['has_unarmed'] = df['armed_with'].str.contains('unarmed', case=False, na=False).astype(int)\n",
        "\n",
        "# Logic update: has_other should NOT include 'unknown' or 'undetermined'\n",
        "other_keywords_for_consolidation = ['other'] # removed unknown and undetermined\n",
        "other_pattern = '|'.join(other_keywords_for_consolidation)\n",
        "df['has_other'] = df['armed_with'].str.contains(other_pattern, case=False, na=False).astype(int)\n",
        "print(f\"4.1 Decomposing the 'armed_with' column: 7 binary weapon features created.\")\n",
        "\n",
        "# 4.2 Race Feature Engineering\n",
        "df['race_std'] = df['race'].astype(str).str.upper().str.strip()\n",
        "is_missing_race = (df['race_std'] == 'NAN') | (df['race'].isna())\n",
        "\n",
        "df['share_white'] = df['race_std'].str.contains('W', case=False, na=False)\n",
        "df['share_black'] = df['race_std'].str.contains('B', case=False, na=False)\n",
        "df['share_native_american'] = df['race_std'].str.contains('N', case=False, na=False)\n",
        "df['share_asian'] = df['race_std'].str.contains('A', case=False, na=False)\n",
        "df['share_hispanic'] = df['race_std'].str.contains('H', case=False, na=False)\n",
        "df['share_two_or_more'] = df['race_std'].str.contains(r'[;,]', regex=True, na=False)\n",
        "\n",
        "df['share_missing'] = is_missing_race.astype(int)\n",
        "\n",
        "race_cols = ['share_white', 'share_black', 'share_native_american',\n",
        "             'share_asian', 'share_hispanic', 'share_two_or_more']\n",
        "\n",
        "for col in race_cols:\n",
        "    df[col] = df[col].astype(int)\n",
        "    df.loc[is_missing_race, col] = 0\n",
        "\n",
        "print(f\"4.2 Race Feature Engineering: Six overlapping binary share features created\")\n",
        "\n",
        "#5. Outlier Treatment Handled\n",
        "df['armed_with'] = df['armed_with'].astype(str)\n",
        "# Added 'unknown' to conflict list just in case, though logically it shouldn't conflict with unarmed if handled correctly\n",
        "weapon_keywords_for_conflict = ['gun', 'knife', 'blunt_object', 'vehicle', 'other', 'replica', 'unknown', 'undetermined']\n",
        "conflict_pattern = r'|'.join(weapon_keywords_for_conflict)\n",
        "rows_to_drop_conflict = df[\n",
        "    df['armed_with'].str.contains(r'unarmed', case=False, na=False) &\n",
        "    df['armed_with'].str.contains(conflict_pattern, case=False, na=False)].index\n",
        "df.drop(rows_to_drop_conflict, inplace=True)\n",
        "print(f\"5.1 Outlier Treatment Handled: Removed {len(rows_to_drop_conflict)} records with conflicting 'unarmed' status\")\n",
        "\n",
        "upper_bound = df['age'].quantile(0.99)\n",
        "df['age'] = df['age'].clip(upper=upper_bound)\n",
        "print(f\"5.2 Outlier Treatment Handled: 'age' capped at 99th percentile ({upper_bound:.1f})\")\n",
        "\n",
        "\n",
        "# Calculate the number of records after cleaning\n",
        "cleaned_rows = len(df)\n",
        "print(f\"Cleaned data volume: {cleaned_rows} records\")\n",
        "\n",
        "# Save the cleaned dataset\n",
        "# [修改说明]: 保存到 Google Drive 的 processed 文件夹\n",
        "output_main_path = os.path.join(processed_data_path, 'cleaned_fatal_shootings_data_2020_2024-newmain.csv')\n",
        "df.to_csv(output_main_path, index=False)\n",
        "print(f\"Cleaned data saved to '{output_main_path}'\")\n",
        "\n",
        "#6. Final Feature Selection\n",
        "columns_to_drop = ['id', 'date','county', 'name', 'race_source', 'location_precision', 'agency_ids']\n",
        "df.drop(columns=columns_to_drop, inplace=True)\n",
        "print(\"6.1 Final Feature Selection: Dropped low-value/redundant columns\")\n",
        "\n",
        "# Calculate the number of records after selected\n",
        "final_rows = len(df)\n",
        "print(f\"Final data volume: {final_rows} records.\")\n",
        "\n",
        "\n",
        "output_selected_path = os.path.join(processed_data_path, 'cleaned_selected_features_2020_2024.csv')\n",
        "df.to_csv(output_selected_path, index=False)\n",
        "print(f\"Final data saved to '{output_selected_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Cleaning code of 'socioeconomic table' from 2020-2024"
      ],
      "metadata": {
        "id": "IKKZZkPQJ1Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/WQD7001_Project'\n",
        "\n",
        "raw_data_dir = os.path.join(base_dir, 'data', 'raw')\n",
        "processed_data_dir = os.path.join(base_dir, 'data', 'processed')\n",
        "\n",
        "os.makedirs(processed_data_dir, exist_ok=True)\n",
        "\n",
        "# Define Census codes to target metric names\n",
        "FINAL_RENAME_MAP = {\n",
        "    'DP03_0128PE': 'poverty_rate',\n",
        "    'DP02_0062PE': 'high_school_grad_rate',\n",
        "    'DP03_0062E': 'median_household_income',\n",
        "    'DP05_0037PE': 'share_white',\n",
        "    'DP05_0038PE': 'share_black','DP05_0045PE': 'share_black',\n",
        "    'DP05_0039PE': 'share_native_american','DP05_0053PE': 'share_native_american',\n",
        "    'DP05_0044PE': 'share_asian', 'DP05_0047PE': 'share_asian', 'DP05_0061PE': 'share_asian',\n",
        "    'DP05_0071PE': 'share_hispanic', 'DP05_0074PE': 'share_hispanic', 'DP05_0076PE': 'share_hispanic', 'DP05_0090PE': 'share_hispanic',\n",
        "    'DP05_0035PE': 'share_two_or_more', 'DP05_0033E': 'total_population',\n",
        "    'state': 'state_code'}\n",
        "\n",
        "# Define auxiliary files and their corresponding years\n",
        "file_maps = [('socioeconomic_2020.csv', 2020), ('socioeconomic_2021.csv', 2021), ('socioeconomic_2022.csv', 2022), ('socioeconomic_2023.csv', 2023), ('socioeconomic_2024.csv', 2024)]\n",
        "\n",
        "all_auxiliary_data = []\n",
        "\n",
        "# State Full Name to Abbreviation Map\n",
        "state_abbr_map = {\n",
        "    'ALABAMA': 'AL', 'ALASKA': 'AK', 'ARIZONA': 'AZ', 'ARKANSAS': 'AR', 'CALIFORNIA': 'CA',\n",
        "    'COLORADO': 'CO', 'CONNECTICUT': 'CT', 'DELAWARE': 'DE', 'DISTRICT OF COLUMBIA': 'DC',\n",
        "    'FLORIDA': 'FL', 'GEORGIA': 'GA', 'HAWAII': 'HI', 'IDAHO': 'ID', 'ILLINOIS': 'IL',\n",
        "    'INDIANA': 'IN', 'IOWA': 'IA', 'KANSAS': 'KS', 'KENTUCKY': 'KY', 'LOUISIANA': 'LA',\n",
        "    'MAINE': 'ME', 'MARYLAND': 'MD', 'MASSACHUSETTS': 'MA', 'MICHIGAN': 'MI', 'MINNESOTA': 'MN',\n",
        "    'MISSISSIPPI': 'MS', 'MISSOURI': 'MO', 'MONTANA': 'MT', 'NEBRASKA': 'NE', 'NEVADA': 'NV',\n",
        "    'NEW HAMPSHIRE': 'NH', 'NEW JERSEY': 'NJ', 'NEW MEXICO': 'NM', 'NEW YORK': 'NY',\n",
        "    'NORTH CAROLINA': 'NC', 'NORTH DAKOTA': 'ND', 'OHIO': 'OH', 'OKLAHOMA': 'OK', 'OREGON': 'OR',\n",
        "    'PENNSYLVANIA': 'PA', 'RHODE ISLAND': 'RI', 'SOUTH CAROLINA': 'SC', 'SOUTH DAKOTA': 'SD',\n",
        "    'TENNESSEE': 'TN', 'TEXAS': 'TX', 'UTAH': 'UT', 'VERMONT': 'VT', 'VIRGINIA': 'VA',\n",
        "    'WASHINGTON': 'WA', 'WEST VIRGINIA': 'WV', 'WISCONSIN': 'WI', 'WYOMING': 'WY',\n",
        "    'PUERTO RICO': 'PR'\n",
        "}\n",
        "\n",
        "print(\"Starting to load auxiliary files...\")\n",
        "\n",
        "for file_name, year in file_maps:\n",
        "    file_path = os.path.join(raw_data_dir, file_name)\n",
        "\n",
        "    try:\n",
        "# 1. Load file with robust encoding\n",
        "        df_temp = pd.read_csv(file_path, encoding='latin-1')\n",
        "        df_temp.columns = df_temp.columns.str.strip()\n",
        "\n",
        "# 2. Select columns and rename\n",
        "        available_cols = ['NAME'] + [col for col in df_temp.columns if col in FINAL_RENAME_MAP or col == 'state']\n",
        "        df_temp = df_temp[available_cols].copy()\n",
        "        df_temp = df_temp.rename(columns=FINAL_RENAME_MAP)\n",
        "        df_temp['year'] = year\n",
        "        all_auxiliary_data.append(df_temp)\n",
        "        print(f\"Loaded {file_name} successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Error: File not found at {file_path}\")\n",
        "\n",
        "# 3. Create the Auxiliary Master Table (Vertical Concatenation)\n",
        "if all_auxiliary_data:\n",
        "    df_auxiliary_master = pd.concat(all_auxiliary_data, ignore_index=True)\n",
        "    print(f\"Auxiliary Master Table created with {len(df_auxiliary_master)} total records.\")\n",
        "\n",
        "    raw_output_path = os.path.join(processed_data_dir, 'auxiliary_socioeconomic_master_table_raw.csv')\n",
        "    df_auxiliary_master.to_csv(raw_output_path, index=False)\n",
        "    print(f\"\\nAuxiliary Master Table successfully saved to '{raw_output_path}'\")\n",
        "\n",
        "    df = pd.read_csv(raw_output_path)\n",
        "\n",
        "    # 4. Create a new column containing the abbreviations for the names of states.\n",
        "    df['state_abbr'] = df['NAME'].astype(str).str.upper().str.strip().map(state_abbr_map)\n",
        "\n",
        "    cols = df.columns.tolist()\n",
        "    if 'NAME' in cols and 'state_abbr' in cols:\n",
        "        name_idx = cols.index('NAME')\n",
        "        cols.insert(name_idx + 1, cols.pop(cols.index('state_abbr')))\n",
        "        df = df[cols]\n",
        "\n",
        "    # 5. Use nan to fill missing value\n",
        "    target_cols = [\n",
        "        'poverty_rate',\n",
        "        'high_school_grad_rate',\n",
        "        'median_household_income',\n",
        "        'share_white',\n",
        "        'share_black',\n",
        "        'share_native_american',\n",
        "        'share_asian',\n",
        "        'share_hispanic',\n",
        "        'share_two_or_more',\n",
        "        'total_population']\n",
        "\n",
        "    missing_values_to_replace = ['', ' ', '#N/A', 'NULL', 'null', 'None', '-']\n",
        "\n",
        "    for col in target_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].replace(missing_values_to_replace, 'nan')\n",
        "            df[col] = df[col].fillna('nan')\n",
        "\n",
        "    # 6. Save final processed file\n",
        "    final_output_path = os.path.join(processed_data_dir, 'auxiliary_socioeconomic_processed.csv')\n",
        "    df.to_csv(final_output_path, index=False)\n",
        "    print(f\"Successfully processed and saved to {final_output_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"No data loaded. Please check your raw data files.\")"
      ],
      "metadata": {
        "id": "geM-PJZVbSPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ccf163-02fd-4ca0-b8fe-bd6717580bc8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Starting to load auxiliary files...\n",
            "Loaded socioeconomic_2020.csv successfully.\n",
            "Loaded socioeconomic_2021.csv successfully.\n",
            "Loaded socioeconomic_2022.csv successfully.\n",
            "Loaded socioeconomic_2023.csv successfully.\n",
            "Loaded socioeconomic_2024.csv successfully.\n",
            "Auxiliary Master Table created with 260 total records.\n",
            "\n",
            "Auxiliary Master Table successfully saved to '/content/drive/MyDrive/WQD7001_Project/data/processed/auxiliary_socioeconomic_master_table_raw.csv'\n",
            "Successfully processed and saved to /content/drive/MyDrive/WQD7001_Project/data/processed/auxiliary_socioeconomic_processed.csv\n"
          ]
        }
      ]
    }
  ]
}